// Smart Complete Tool - Main tool for intelligent model routing

import { z } from 'zod';
import { analyzePrompt, explainAnalysis } from '../lib/prompt-analyzer.js';
import { selectModel, SelectionPreferences } from '../lib/model-selector.js';
import { createGoogleAIClient } from '../lib/api-clients/google-ai-client.js';
import { getModelById } from '../lib/model-registry.js';

// Input schema
export const SmartCompleteSchema = z.object({
    prompt: z.string().min(1, 'Prompt is required'),
    context: z.string().optional(),
    preferences: z.object({
        prefer_speed: z.boolean().optional(),
        prefer_cost: z.boolean().optional(),
        prefer_quality: z.boolean().optional(),
        max_cost_per_request: z.number().optional(),
    }).optional(),
    system_instruction: z.string().optional(),
    max_tokens: z.number().optional(),
    temperature: z.number().min(0).max(2).optional(),
});

export type SmartCompleteInput = z.infer<typeof SmartCompleteSchema>;

export interface SmartCompleteOutput {
    response: string;
    metadata: {
        model_used: string;
        model_display_name: string;
        complexity: string;
        task_type: string;
        reason: string;
        cost_estimate: number;
        tokens_used: {
            input: number;
            output: number;
        };
        savings_vs_premium?: number;
        analysis_confidence: number;
    };
}

/**
 * Smart Complete - Automatically selects and calls the best AI model
 */
export async function smartComplete(input: SmartCompleteInput): Promise<SmartCompleteOutput> {
    const { prompt, context, preferences, system_instruction, max_tokens, temperature } = input;

    // Step 1: Analyze the prompt
    const analysis = analyzePrompt(prompt, context);
    console.log('Prompt analysis:', explainAnalysis(analysis));

    // Step 2: Select the optimal model
    const selectionPreferences: SelectionPreferences = {
        preferSpeed: preferences?.prefer_speed,
        preferCost: preferences?.prefer_cost,
        preferQuality: preferences?.prefer_quality,
        maxCostPerRequest: preferences?.max_cost_per_request,
    };

    let selection = selectModel(analysis, selectionPreferences);
    console.log(`Selected model: ${selection.model.displayName} - ${selection.reason}`);

    // Step 3: Call the appropriate API based on model provider
    let response: string = "";
    let tokensUsed = { input: 0, output: 0 };
    let isSimulated = false;

    try {
        if (selection.model.provider === 'google') {
            const googleClient = createGoogleAIClient();
            if (!googleClient) throw new Error('Google AI client not available.');

            if (analysis.taskType === 'image') {
                // Image generation
                const imageResult = await googleClient.generateImage({ prompt });
                response = `Generated image(s):\n\n${imageResult.images.map((img, i) => `![Image ${i + 1}](${img.url})`).join('\n\n')}`;
                tokensUsed = { input: 100, output: 1000 };
            } else {
                // Text generation
                const textResult = await googleClient.generateText({
                    model: selection.model.name,
                    prompt,
                    systemInstruction: system_instruction,
                    maxTokens: max_tokens,
                    temperature,
                });

                response = textResult.text;
                tokensUsed = {
                    input: textResult.usage.inputTokens,
                    output: textResult.usage.outputTokens,
                };
            }
        } else {
            throw new Error(`Unsupported model provider: ${selection.model.provider}`);
        }
    } catch (error: any) {
        console.error("API Call failed:", error.message);

        // Fallback Logic for 429/Quota Errors
        // If Google blocks us, we SIMULATE the response so you can still verify FluxAI logic
        if (error.message && (error.message.includes('429') || error.message.includes('quota') || error.message.includes('RESOURCE_EXHAUSTED') || error.message.includes('400'))) {
            console.warn(`Primary model exhausted. Returning SIMULATED response for testing.`);
            isSimulated = true;

            response = `[SYSTEM: API QUOTA EXHAUSTED - SIMULATED RESPONSE]\n\nSince your Google AI free tier is exhausted, FluxAI is simulating this response to demonstrate logic.\n\n**Architecture Analysis for: ${prompt.substring(0, 50)}...**\n\n1. **Microservices Pattern**: Recommended for scalability.\n2. **Database**: Use sharded PostgreSQL for transactional integrity.\n3. **Security**: Implement mTLS for service-to-service auth.\n\n(This text was not generated by the LLM, but the routing logic below is real)`;

            // Mock token usage based on prompt length
            tokensUsed = {
                input: Math.ceil(prompt.length / 4),
                output: 300
            };
        } else {
            throw error;
        }
    }

    // Step 4: Calculate actual cost
    const actualCost = calculateCost(
        selection.model.pricing.inputCostPer1MTokens,
        selection.model.pricing.outputCostPer1MTokens,
        tokensUsed.input,
        tokensUsed.output
    );

    // Step 5: Calculate savings vs premium model
    const premiumModel = getModelById('gemini-2.0-pro-experimental');
    let savingsVsPremium: number | undefined;

    if (premiumModel && selection.model.id !== premiumModel.id) {
        const premiumCost = calculateCost(
            premiumModel.pricing.inputCostPer1MTokens,
            premiumModel.pricing.outputCostPer1MTokens,
            tokensUsed.input,
            tokensUsed.output
        );
        savingsVsPremium = premiumCost - actualCost;
    }

    return {
        response,
        metadata: {
            model_used: selection.model.id,
            model_display_name: `${selection.model.displayName}${isSimulated ? ' (SIMULATED)' : ''}`,
            complexity: analysis.complexity,
            task_type: analysis.taskType,
            reason: isSimulated ? 'Quota exceeded - simulated execution' : selection.reason,
            cost_estimate: actualCost,
            tokens_used: tokensUsed,
            savings_vs_premium: savingsVsPremium,
            analysis_confidence: analysis.confidence,
        },
    };
}

function calculateCost(
    inputCostPer1M: number,
    outputCostPer1M: number,
    inputTokens: number,
    outputTokens: number
): number {
    const inputCost = (inputTokens / 1_000_000) * inputCostPer1M;
    const outputCost = (outputTokens / 1_000_000) * outputCostPer1M;
    return inputCost + outputCost;
}